{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference with Pyoso\n",
    "\n",
    "Welcome to this tutorial on using causal inference with pyoso! This tutorial will guide you through the process of using synthetic control methods to analyze the causal impact of events or interventions on open source projects using data from OSO (Open Source Observer).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Causal inference goes beyond correlation to understand cause-and-effect relationships. While randomized experiments are the gold standard for establishing causation, they aren't always possible in the context of open source software development. This is where observational causal inference methods become valuable.\n",
    "\n",
    "In this tutorial, we'll explore:\n",
    "\n",
    "1. Introduction to causal inference and why it matters for open source metrics\n",
    "2. Setting up your environment with pyoso\n",
    "3. Understanding synthetic control methods\n",
    "4. Implementing a synthetic control analysis to measure the impact of grant funding on open source projects\n",
    "5. Interpreting and visualizing your results\n",
    "6. Best practices and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Causal Inference\n",
    "\n",
    "Causal inference is the process of determining whether a specific intervention or treatment had an effect on an outcome of interest. The fundamental challenge of causal inference is that we can never observe the counterfactual - what would have happened if the treatment had not been applied.\n",
    "\n",
    "In open source software metrics, we might want to answer questions like:\n",
    "\n",
    "- Did receiving a grant increase the number of active developers contributing to a project?\n",
    "- Does a specific contribution behavior (like posting) lead to increased long-term engagement?\n",
    "- What impact did a specific event (like a hackathon) have on project activity?\n",
    "\n",
    "These questions are inherently causal. Simple correlational analysis might lead to misleading conclusions because of confounding factors - variables that affect both the treatment and the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment\n",
    "\n",
    "First, let's install the necessary packages and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pyoso pysyncon numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyoso import Client\n",
    "import pysyncon as psc\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OSO with your API key\n",
    "# If you're running this in Google Colab, you can use userdata.get('OSO_API_KEY')\n",
    "# If running locally, you can use environment variables or paste your key directly (not recommended for shared notebooks)\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OSO_API_KEY = userdata.get('OSO_API_KEY')\n",
    "except Exception:\n",
    "    OSO_API_KEY = os.environ.get('OSO_API_KEY')  # Set your API key as an environment variable\n",
    "\n",
    "# Initialize the client\n",
    "client = Client(api_key=OSO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Synthetic Control Methods\n",
    "\n",
    "### What are Synthetic Control Methods?\n",
    "\n",
    "The synthetic control method is a statistical technique used to evaluate the effect of an intervention in comparative case studies. It creates a weighted combination of control units (the \"synthetic control\") that best resembles the characteristics of the treated unit before the intervention.\n",
    "\n",
    "The key idea is to use data from before the intervention to create a synthetic version of the treated unit, and then compare the post-intervention outcomes between the actual treated unit and its synthetic counterpart. The difference between these two represents the causal effect of the intervention.\n",
    "\n",
    "### When to Use Synthetic Control Methods?\n",
    "\n",
    "Synthetic control methods are particularly useful when:\n",
    "\n",
    "1. You have a small number of treated units (often just one)\n",
    "2. You have several potential control units\n",
    "3. You have data for time periods before and after the intervention\n",
    "4. Randomized experiments are not feasible\n",
    "\n",
    "This makes them perfect for analyzing the impact of events or interventions on open source projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing a Synthetic Control Analysis\n",
    "\n",
    "Let's use a concrete example: analyzing the impact of receiving a grant on the number of active developers contributing to a project.\n",
    "\n",
    "### Step 1: Define the research question\n",
    "\n",
    "We want to know: **Did receiving a grant increase the number of active developers for a project?**\n",
    "\n",
    "### Step 2: Identify the treated and potential control units\n",
    "\n",
    "- Treated unit: A project that received a grant at a specific point in time\n",
    "- Potential control units: Similar projects that did not receive grants\n",
    "\n",
    "### Step 3: Collect data on the outcome variable and predictors before and after the intervention\n",
    "\n",
    "Let's query OSO data to get timeseries metrics for active developers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our helper function for formatting arrays in SQL queries\n",
    "def stringify(arr):\n",
    "    return \"'\" + \"','\".join(arr) + \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, let's assume a project called 'project-xyz' received a grant in January 2024\n",
    "# We'll compare it with other similar projects that didn't receive grants\n",
    "\n",
    "# First, let's identify our treated project and potential control projects\n",
    "treated_project = 'project-xyz'  # Replace with an actual project name from the OSO database\n",
    "potential_control_projects = ['project-a', 'project-b', 'project-c', 'project-d', 'project-e']\n",
    "\n",
    "# Let's get active developer data for these projects\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  p.project_name,\n",
    "  tm.sample_date,\n",
    "  tm.amount as active_devs_90d\n",
    "FROM timeseries_metrics_by_project_v0 AS tm\n",
    "JOIN metrics_v0 AS m ON m.metric_id = tm.metric_id\n",
    "JOIN projects_v1 AS p ON p.project_id = tm.project_id\n",
    "WHERE\n",
    "  p.project_name IN ({stringify([treated_project] + potential_control_projects)})\n",
    "  AND m.metric_name = 'active_developers_over_90_day'\n",
    "  AND tm.sample_date BETWEEN '2023-01-01' AND '2024-06-30'\n",
    "ORDER BY p.project_name, tm.sample_date\n",
    "\"\"\"\n",
    "\n",
    "df_devs = client.to_pandas(query)\n",
    "df_devs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also get other metrics that might be good predictors\n",
    "predictors = ['commits_over_90_day', 'issues_opened_over_90_day', 'new_contributors_over_90_day']\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  p.project_name,\n",
    "  m.metric_name,\n",
    "  tm.sample_date,\n",
    "  tm.amount\n",
    "FROM timeseries_metrics_by_project_v0 AS tm\n",
    "JOIN metrics_v0 AS m ON m.metric_id = tm.metric_id\n",
    "JOIN projects_v1 AS p ON p.project_id = tm.project_id\n",
    "WHERE\n",
    "  p.project_name IN ({stringify([treated_project] + potential_control_projects)})\n",
    "  AND m.metric_name IN ({stringify(predictors)})\n",
    "  AND tm.sample_date BETWEEN '2023-01-01' AND '2024-06-30'\n",
    "ORDER BY p.project_name, m.metric_name, tm.sample_date\n",
    "\"\"\"\n",
    "\n",
    "df_predictors = client.to_pandas(query)\n",
    "df_predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predictor data from long to wide format\n",
    "df_predictors_wide = df_predictors.pivot_table(\n",
    "    index=['project_name', 'sample_date'],\n",
    "    columns='metric_name',\n",
    "    values='amount'\n",
    ").reset_index()\n",
    "\n",
    "# Merge with our active developer data\n",
    "df_combined = pd.merge(\n",
    "    df_devs,\n",
    "    df_predictors_wide,\n",
    "    on=['project_name', 'sample_date'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the pre-intervention and post-intervention periods\n",
    "\n",
    "For our example, let's assume the project received a grant on January 1, 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intervention date\n",
    "intervention_date = '2024-01-01'\n",
    "\n",
    "# Plot the data for the treated project with a vertical line at the intervention date\n",
    "plt.figure(figsize=(14, 8))\n",
    "for project in [treated_project] + potential_control_projects:\n",
    "    project_data = df_combined[df_combined['project_name'] == project]\n",
    "    plt.plot(project_data['sample_date'], project_data['active_devs_90d'], label=project)\n",
    "\n",
    "plt.axvline(x=pd.to_datetime(intervention_date), color='r', linestyle='--', alpha=0.7, label='Intervention (Grant)')\n",
    "plt.title('Active Developers (90-day) Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Active Developers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Prepare the data for pysyncon\n",
    "\n",
    "The pysyncon package requires data in a specific format. Let's prepare our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for pysyncon\n",
    "# First, reshape the data to have dates as rows and projects as columns\n",
    "pivot_df = df_combined.pivot(index='sample_date', columns='project_name', values='active_devs_90d')\n",
    "pivot_df.index = pd.to_datetime(pivot_df.index)\n",
    "pivot_df = pivot_df.sort_index()\n",
    "\n",
    "# Prepare predictor variables similarly\n",
    "predictor_dfs = {}\n",
    "for predictor in predictors:\n",
    "    pred_pivot = df_combined.pivot(index='sample_date', columns='project_name', values=predictor)\n",
    "    pred_pivot.index = pd.to_datetime(pred_pivot.index)\n",
    "    pred_pivot = pred_pivot.sort_index()\n",
    "    predictor_dfs[predictor] = pred_pivot\n",
    "\n",
    "# Convert to numpy arrays for pysyncon\n",
    "treated_outcome = pivot_df[treated_project].values\n",
    "control_outcomes = pivot_df.drop(columns=[treated_project]).values\n",
    "control_names = pivot_df.drop(columns=[treated_project]).columns.tolist()\n",
    "\n",
    "# Find the index of the intervention date\n",
    "intervention_idx = pivot_df.index.get_loc(pd.to_datetime(intervention_date))\n",
    "\n",
    "# Extract pre-intervention and post-intervention periods\n",
    "pre_period = (0, intervention_idx - 1)  # Index 0 to the day before intervention\n",
    "post_period = (intervention_idx, len(pivot_df) - 1)  # Intervention day to the end\n",
    "\n",
    "# Prepare predictor matrices\n",
    "predictor_matrices = []\n",
    "for predictor in predictors:\n",
    "    pred_treated = predictor_dfs[predictor][treated_project].values\n",
    "    pred_controls = predictor_dfs[predictor].drop(columns=[treated_project]).values\n",
    "    predictor_matrices.append((pred_treated, pred_controls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run the synthetic control analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the synthetic control model\n",
    "synth = psc.psc.Synth()\n",
    "synth.fit(\n",
    "    X1=treated_outcome[pre_period[0]:pre_period[1]+1],  # Pre-intervention treated outcome\n",
    "    X0=control_outcomes[pre_period[0]:pre_period[1]+1],  # Pre-intervention control outcomes\n",
    "    Z1=[p[0][pre_period[0]:pre_period[1]+1] for p in predictor_matrices],  # Pre-intervention treated predictors\n",
    "    Z0=[p[1][pre_period[0]:pre_period[1]+1] for p in predictor_matrices],  # Pre-intervention control predictors\n",
    ")\n",
    "\n",
    "# Get the weights assigned to each control unit\n",
    "synth_weights = synth.weights\n",
    "synth_control_weights = pd.DataFrame({'Project': control_names, 'Weight': synth_weights})\n",
    "synth_control_weights = synth_control_weights.sort_values('Weight', ascending=False)\n",
    "\n",
    "# Display the weights\n",
    "print(\"Synthetic Control Weights:\")\n",
    "print(synth_control_weights)\n",
    "\n",
    "# Plot the weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Weight', y='Project', data=synth_control_weights, palette='viridis')\n",
    "plt.title('Synthetic Control Weights by Project')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the synthetic control series\n",
    "synthetic_outcome = np.dot(control_outcomes, synth_weights)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': pivot_df.index,\n",
    "    'Actual': treated_outcome,\n",
    "    'Synthetic': synthetic_outcome,\n",
    "    'Gap': treated_outcome - synthetic_outcome\n",
    "})\n",
    "\n",
    "# Mark pre and post intervention periods\n",
    "results_df['Period'] = 'Pre-intervention'\n",
    "results_df.loc[results_df['Date'] >= pd.to_datetime(intervention_date), 'Period'] = 'Post-intervention'\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(results_df['Date'], results_df['Actual'], 'b-', label='Actual')\n",
    "plt.plot(results_df['Date'], results_df['Synthetic'], 'r--', label='Synthetic Control')\n",
    "plt.axvline(x=pd.to_datetime(intervention_date), color='green', linestyle='--', alpha=0.7, label='Intervention (Grant)')\n",
    "plt.title(f'Synthetic Control Analysis: Impact of Grant on {treated_project}')\n",
    "plt.ylabel('Active Developers (90-day)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot the gap (treatment effect)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(results_df['Date'], results_df['Gap'], alpha=0.7, label='Treatment Effect (Gap)')\n",
    "plt.axvline(x=pd.to_datetime(intervention_date), color='green', linestyle='--', alpha=0.7, label='Intervention (Grant)')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.title('Estimated Treatment Effect (Gap between Actual and Synthetic Control)')\n",
    "plt.ylabel('Effect Size')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the fit and calculate the treatment effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean pre-intervention fit\n",
    "pre_MSE = np.mean((results_df[results_df['Period'] == 'Pre-intervention']['Actual'] - \n",
    "                   results_df[results_df['Period'] == 'Pre-intervention']['Synthetic']) ** 2)\n",
    "pre_RMSE = np.sqrt(pre_MSE)\n",
    "print(f\"Pre-intervention RMSE: {pre_RMSE:.4f}\")\n",
    "\n",
    "# Calculate the average treatment effect in the post-intervention period\n",
    "average_treatment_effect = np.mean(results_df[results_df['Period'] == 'Post-intervention']['Gap'])\n",
    "print(f\"Average Treatment Effect: {average_treatment_effect:.4f} additional active developers\")\n",
    "\n",
    "# Calculate the percentage change\n",
    "average_synthetic = np.mean(results_df[results_df['Period'] == 'Post-intervention']['Synthetic'])\n",
    "percentage_change = (average_treatment_effect / average_synthetic) * 100\n",
    "print(f\"Percentage Change: {percentage_change:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Conduct placebo tests to assess the significance of our findings\n",
    "\n",
    "To evaluate whether our results could have occurred by chance, we can conduct placebo tests by applying the same analysis to each control unit as if it had received the treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run placebo test for a single control unit\n",
    "def run_placebo_test(control_idx, treated_outcome, control_outcomes, predictor_matrices, pre_period, post_period):\n",
    "    # Extract the control unit to be treated as \"treated\"\n",
    "    placebo_treated = control_outcomes[:, control_idx]\n",
    "    \n",
    "    # Create a new control set excluding the placebo treated unit\n",
    "    placebo_controls = np.delete(control_outcomes, control_idx, axis=1)\n",
    "    \n",
    "    # Add the original treated unit to the control set\n",
    "    placebo_controls = np.column_stack((treated_outcome, placebo_controls))\n",
    "    \n",
    "    # Adjust predictor matrices\n",
    "    placebo_predictor_matrices = []\n",
    "    for p_treated, p_controls in predictor_matrices:\n",
    "        placebo_p_treated = p_controls[:, control_idx]\n",
    "        placebo_p_controls = np.delete(p_controls, control_idx, axis=1)\n",
    "        placebo_p_controls = np.column_stack((p_treated, placebo_p_controls))\n",
    "        placebo_predictor_matrices.append((placebo_p_treated, placebo_p_controls))\n",
    "    \n",
    "    # Create and fit the synthetic control model\n",
    "    placebo_synth = psc.psc.Synth()\n",
    "    try:\n",
    "        placebo_synth.fit(\n",
    "            X1=placebo_treated[pre_period[0]:pre_period[1]+1],\n",
    "            X0=placebo_controls[pre_period[0]:pre_period[1]+1],\n",
    "            Z1=[p[0][pre_period[0]:pre_period[1]+1] for p in placebo_predictor_matrices],\n",
    "            Z0=[p[1][pre_period[0]:pre_period[1]+1] for p in placebo_predictor_matrices],\n",
    "        )\n",
    "        \n",
    "        # Calculate the synthetic control series\n",
    "        placebo_synthetic = np.dot(placebo_controls, placebo_synth.weights)\n",
    "        \n",
    "        # Calculate the gap (effect)\n",
    "        placebo_gap = placebo_treated - placebo_synthetic\n",
    "        \n",
    "        return placebo_gap\n",
    "    except Exception:\n",
    "        # If the placebo test fails, return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run placebo tests for all control units\n",
    "placebo_gaps = []\n",
    "placebo_labels = []\n",
    "\n",
    "# Real treated unit first\n",
    "placebo_gaps.append(results_df['Gap'].values)\n",
    "placebo_labels.append(treated_project)\n",
    "\n",
    "# Then all placebo units\n",
    "for i in range(len(control_names)):\n",
    "    gap = run_placebo_test(i, treated_outcome, control_outcomes, predictor_matrices, pre_period, post_period)\n",
    "    if gap is not None:\n",
    "        placebo_gaps.append(gap)\n",
    "        placebo_labels.append(control_names[i])\n",
    "\n",
    "# Plot all gaps\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, gap in enumerate(placebo_gaps):\n",
    "    if placebo_labels[i] == treated_project:\n",
    "        plt.plot(results_df['Date'], gap, 'b-', linewidth=2, label=placebo_labels[i] + \" (Treated)\")\n",
    "    else:\n",
    "        plt.plot(results_df['Date'], gap, 'gray', alpha=0.5, linewidth=1, label=placebo_labels[i])\n",
    "\n",
    "plt.axvline(x=pd.to_datetime(intervention_date), color='green', linestyle='--', alpha=0.7, label='Intervention (Grant)')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.title('Placebo Tests: Gap between Actual and Synthetic Control for All Units')\n",
    "plt.ylabel('Gap (Effect Size)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpreting and Visualizing the Results\n",
    "\n",
    "Let's interpret the results of our synthetic control analysis:\n",
    "\n",
    "1. **Pre-intervention fit**: The RMSE in the pre-intervention period tells us how well our synthetic control matches the treated unit before the intervention. A low RMSE indicates a good fit.\n",
    "\n",
    "2. **Average Treatment Effect**: This represents the average difference between the actual outcome and the synthetic control in the post-intervention period. If positive, it suggests the intervention had a positive effect.\n",
    "\n",
    "3. **Percentage Change**: This gives us the relative size of the effect compared to what would have happened without the intervention.\n",
    "\n",
    "4. **Placebo Tests**: By comparing the gap for our treated unit with placebo gaps for control units, we can assess whether the observed effect is significantly different from what we might observe by chance. If the treated unit's gap is consistently larger than the placebo gaps, it supports the conclusion that the intervention had a causal effect.\n",
    "\n",
    "### Causal Interpretation\n",
    "\n",
    "Based on our analysis, we can conclude whether receiving a grant causally impacted the number of active developers for our project. However, some important caveats to consider:\n",
    "\n",
    "1. **Assumptions**: The synthetic control method assumes that the relationship between the treated unit and controls in the pre-intervention period would have continued in the absence of treatment.\n",
    "\n",
    "2. **Confounding Events**: If other events occurred around the same time as our intervention and affected only the treated unit, they could confound our results.\n",
    "\n",
    "3. **Extrapolation**: We should be cautious about extrapolating the results too far into the future.\n",
    "\n",
    "4. **Number of Control Units**: The reliability of the synthetic control increases with the number of suitable control units available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Limitations\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Choose predictors wisely**: Select predictors that are likely to influence the outcome variable but aren't affected by the intervention.\n",
    "\n",
    "2. **Check the pre-intervention fit**: A good synthetic control should closely match the treated unit in the pre-intervention period.\n",
    "\n",
    "3. **Conduct placebo tests**: These help assess whether the observed effect could have occurred by chance.\n",
    "\n",
    "4. **Consider multiple periods**: If possible, examine the effect over different time horizons to see if it's stable.\n",
    "\n",
    "5. **Be transparent about limitations**: Acknowledge the assumptions and potential confounders in your analysis.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Cannot handle multiple treated units easily**: The standard synthetic control method is designed for a single treated unit.\n",
    "\n",
    "2. **Requires a reasonable number of pre-intervention periods**: Without sufficient pre-intervention data, it's hard to create a good synthetic control.\n",
    "\n",
    "3. **Assumes no spillover effects**: The method assumes the intervention doesn't affect the control units.\n",
    "\n",
    "4. **Selection of control units is crucial**: The quality of the synthetic control depends on having suitable control units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "\n",
    "1. Connect to OSO data using pyoso\n",
    "2. Understand the principles of causal inference and synthetic control methods\n",
    "3. Prepare data for a synthetic control analysis\n",
    "4. Implement a synthetic control analysis using pysyncon\n",
    "5. Interpret and visualize the results\n",
    "6. Evaluate the robustness of our findings through placebo tests\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further explore causal inference with OSO data, you might want to:\n",
    "\n",
    "1. **Try different outcome variables**: Beyond active developers, consider other metrics like commits, issues, or user engagement.\n",
    "\n",
    "2. **Explore different interventions**: Such as marketing campaigns, hackathons, or new feature releases.\n",
    "\n",
    "3. **Implement other causal inference methods**: Such as difference-in-differences, regression discontinuity, or instrumental variables.\n",
    "\n",
    "4. **Combine with other data sources**: Integrate OSO data with other sources for a more comprehensive analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
